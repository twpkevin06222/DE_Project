{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevinteng/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/kevinteng/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/kevinteng/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/kevinteng/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/kevinteng/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/kevinteng/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('/home/kevinteng/Desktop/DE_Project')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, Flatten, Dense, Softmax, MaxPooling2D\n",
    "from coord_conv import CoordConv\n",
    "import utils\n",
    "import utils_vis\n",
    "import utils_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_regression(inps): \n",
    "    coord01 = CoordConv(x_dim = 100, y_dim = 100, with_r = False, filters = 20, \n",
    "              kernel_size = 1)(inps)\n",
    "    conv01 = Conv2D(20,1, activation = None)(coord01) \n",
    "    conv02 = Conv2D(20,1, activation = None)(conv01)\n",
    "    conv03 = Conv2D(20,1, activation = None)(conv02)\n",
    "    conv04 = Conv2D(20,1, activation = None)(conv03) \n",
    "    conv05 = Conv2D(20,1, activation = None)(conv04) \n",
    "    conv06 = Conv2D(20,3, activation = None)(conv05) \n",
    "    conv07 = Conv2D(2,3 ,activation = None)(conv06) \n",
    "    output = MaxPooling2D( pool_size= 96, strides = 96, padding='valid')(conv07)\n",
    "    return output \n",
    "\n",
    "def de_conv(input_tensor):\n",
    "    coord01 = CoordConv(x_dim = 100, y_dim = 100, with_r = False, filters = 100,\n",
    "              kernel_size = 1, padding='same', activation='relu')(input_tensor)\n",
    "    conv01 = Conv2D(filters = 50, kernel_size = 1, strides = 1, padding = \"same\", activation = 'relu')(coord01)\n",
    "    conv02 = Conv2D(filters = 50, kernel_size = 1, strides = 1, padding = \"same\", activation = 'relu')(conv01)\n",
    "    conv03 = Conv2D(filters = 100, kernel_size = 1, strides = 1, padding = \"same\", activation = 'relu')(conv02)\n",
    "    conv04 = Conv2D(filters = 100, kernel_size = 1, strides = 1, padding = \"same\", activation = 'relu')(conv03)\n",
    "    conv05 = Conv2D(filters = 1, kernel_size = 1, strides = 1, padding = \"same\" )(conv04)\n",
    "    f1 = Flatten()(conv05)\n",
    "    output = Softmax(axis = -1)(f1)\n",
    "    return output\n",
    "\n",
    "def latent_space(encoded_imgs, deconv_imgs, coords, batch_size, img_size):\n",
    "    '''\n",
    "    This function construct the dot product of the output of an encoder(batch_size, img_size, img_size, 1) \n",
    "    with the one hot images generated by decoord-conv(n_neurons, img_size, img_size, 1), which yields\n",
    "    similarity score (batch_size, n_neurons). Similarity score is then concatenate to the last layer of the \n",
    "    input coordinates with the corresponding batch (batch_size, n_neurons, 3)\n",
    "    \n",
    "    @encoded_imgs: output images from the encoder\n",
    "    @deconv_imgs: one hot images generated from the coordinate list \n",
    "    @coords: A list of coordinates generated randomly based on the number of neurons\n",
    "    @batch_size: Batch size of the input image for encoder, to allocate size for writing in loop\n",
    "    \n",
    "    return: \n",
    "        latent space with dim (batch_size, n_neurons, 3, 1)\n",
    "    '''\n",
    "    similarity_score = tf.tensordot(encoded_imgs, deconv_imgs, [[1,2,3],[1,2,3]]) #(batch_size, n_neurons)\n",
    "    squeeze_coords = tf.squeeze(coords/(img_size-1)) #coords dim (n_neurons, 1, 1, 2) = > (n_neurons, 2)\n",
    "    latent = tf.TensorArray(tf.float32, size=0, dynamic_size=True)\n",
    "    for i in tf.range(batch_size):\n",
    "        concat = tf.stack((squeeze_coords[:,-2], squeeze_coords[:,-1], similarity_score[i]), axis=-1)    \n",
    "        latent = latent.write(i, concat) #(batch_size, n_neurons, 3)\n",
    "    return tf.expand_dims(latent.stack(), axis = -1), similarity_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_nuerons' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-0e8d439bfef8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mneuron_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mutils_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneuron_like_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_nuerons\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMG_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mneuron_imgs_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneuron_imgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mneuron_imgs_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneuron_imgs_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'n_nuerons' is not defined"
     ]
    }
   ],
   "source": [
    "n_neurons = 2\n",
    "IMG_SIZE = 100\n",
    "BATCH_SIZE = 8\n",
    "lr=0.1\n",
    "\n",
    "neuron_imgs_list = []\n",
    "coords,_, ori_one_hot_imgs = utils_model.create_dat_samples(n_neurons, IMG_SIZE)\n",
    "coords = tf.reshape(coords, [n_neurons, 1,1,2]) #for the input shape of deconv \n",
    "\n",
    "for i in range(BATCH_SIZE):\n",
    "    neuron_imgs = tf.squeeze(utils_model.neuron_like_image(n_neurons, IMG_SIZE))\n",
    "    neuron_imgs_list.append(neuron_imgs)\n",
    "neuron_imgs_list = tf.expand_dims(tf.convert_to_tensor(neuron_imgs_list), -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build coord-deconv model\n",
    "inp = Input(shape=(100, 100,2)) #(batch_size, img_size, img_size, 2)\n",
    "de_coordconv = Model(inp, de_conv(inp))\n",
    "de_coordconv.load_weights(\"best_class_model100_02.hdf5\")\n",
    "\n",
    "#build reg model\n",
    "model_input = Input(shape=(100,100,1))\n",
    "coordconv = Model(model_input, model_regression(model_input))\n",
    "coordconv.load_weights(\"best_reg_model100_02.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss function \n",
    "mse = tf.losses.MeanSquaredError()\n",
    "def test_fn(inp_imgs, coords, img_size):\n",
    "    norm_coords = coords/(img_size-1)\n",
    "    m1 = tf.constant([1,img_size, img_size,1], tf.int32) #multiplier for tiling \n",
    "    tile_cd = tf.tile(norm_coords, m1)\n",
    "    #deconv\n",
    "    de_coordconv.trainable = False #freeze de-coordconv\n",
    "    one_hot_imgs = tf.reshape(de_coordconv(tile_cd), [-1, img_size, img_size, 1])\n",
    "    #dot product\n",
    "    similarity_score = tf.tensordot(inp_imgs, one_hot_imgs, [[1,2,3],[1,2,3]])\n",
    "    #multiply similarity scores with one hot images\n",
    "    similarity_multi = tf.einsum('ij,jklm->ijklm', similarity_score, one_hot_imgs) #(batch_size, n_neurons, img_size, img_size,1)\n",
    "    #sum the one hot images for all neurons to compute MSE\n",
    "    similarity_multi_reduced_neurons = tf.reduce_sum(similarity_multi, axis=1)#(batch_size, img_size, img_size, 1)\n",
    "    similarity_multi_max_batch = tf.reduce_max(similarity_multi, axis = 0)#(n_neurons,img_size,mg_size,1)\n",
    "    #generate coordinates from coordconv model \n",
    "    coordconv.trainable = True \n",
    "    generate_coords = coordconv(similarity_multi_max_batch)\n",
    "    #reconstruciton loss \n",
    "    loss = mse(inp_imgs, similarity_multi_reduced_neurons)\n",
    "    \n",
    "    return loss, one_hot_imgs, generate_coords, similarity_multi_reduced_neurons, similarity_multi_max_batch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, one_hot_imgs, generate_coords, similarity_multi_reduced_neurons, similarity_multi_max_batch  = test_fn(neuron_imgs_list, coords, IMG_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(tf.squeeze(neuron_imgs_list[0]), cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(tf.squeeze(np.sum(similarity_multi_max_batch, axis=0)), cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(tf.squeeze(similarity_multi_reduced_neurons[1]), cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer \n",
    "opt= tf.keras.optimizers.Adam(lr)\n",
    "#loss function \n",
    "mse = tf.losses.MeanSquaredError()\n",
    "@tf.function\n",
    "def train_fn(inp_imgs, coords, img_size):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        #normalizing the coordinates \n",
    "        norm_coords = coords/(img_size-1)\n",
    "        m1 = tf.constant([1,img_size, img_size,1], tf.int32) #multiplier for tiling \n",
    "        tile_cd = tf.tile(norm_coords, m1)\n",
    "        #deconv\n",
    "        de_coordconv.trainable = False #freeze de-coordconv\n",
    "        one_hot_imgs = tf.reshape(de_coordconv(tile_cd), [-1, img_size, img_size, 1])\n",
    "        #dot product\n",
    "        similarity_score = tf.tensordot(inp_imgs, one_hot_imgs, [[1,2,3],[1,2,3]])\n",
    "        #multiply similarity scores with one hot images\n",
    "        similarity_multi = tf.einsum('ij,jklm->ijklm', similarity_score, one_hot_imgs) #(batch_size, n_neurons, img_size, img_size,1)\n",
    "        #sum the one hot images for all neurons to compute MSE\n",
    "        similarity_multi_reduced_neurons = tf.math.reduce_sum(similarity_multi, axis=1)#(batch_size, img_size, img_size, 1)\n",
    "        similarity_multi_max_batch = tf.math.reduce_max(similarity_multi, axis = 0)#(n_neurons,img_size,mg_size,1)\n",
    "        #reconstruciton loss \n",
    "        loss = mse(inp_imgs, similarity_multi_reduced_neurons)\n",
    "        #generate coordinates from coordconv model \n",
    "        coordconv.trainable = True \n",
    "        generate_coords = coordconv(similarity_multi_max_batch)\n",
    "\n",
    "    #back prop through coordconv\n",
    "    grad = tape.gradient(loss, coordconv.trainable_variables)\n",
    "    opt.apply_gradients(zip(grad, coordconv.trainable_variables))\n",
    "    \n",
    "    return loss, generate_coords, one_hot_imgs, similarity_score, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 0\n",
    "while steps<=1000:\n",
    "    if steps==0:\n",
    "        updated_coords = coords\n",
    "    loss, generate_coords, one_hot_imgs, similarity_score, grad = train_fn(neuron_imgs_list, updated_coords, IMG_SIZE)\n",
    "    updated_coords = generate_coords\n",
    "    \n",
    "    print(\"Loss: {}\".format(loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
